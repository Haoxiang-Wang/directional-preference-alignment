# Directional Preference Alignment

This is the repo for paper "**Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards**" by Haoxiang Wang*, Yong Lin*, Wei Xiong*, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang

**Code**: Will be released soon. Stay tuned! 

**Model**: https://huggingface.co/Haoxiang-Wang/DPA-v1-Mistral-7B/
